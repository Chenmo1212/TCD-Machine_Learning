# 线性回归与梯度下降

## 引例

假设银行贷款系统主要考察贷款人的工资，依据工资来决定贷款人可以贷款多少钱。已知现在存在一批历史银行贷款的数据，现在要求对于指定的工资的人，预测出银行可以贷款给多少钱。

## 1. 回归算法(Regression algorithm)

这里的`回归算法`是相对于`分类算法`而言的，是属于机器学习中对算法的一种分类。依据分类标准不同，可以将算法分类成不同的算法。

- 回归算法：通常目标变量是连续型变量，如预测年龄（13，35，78）、收入（1w、2w)等，此时需要用回归算法去拟合得出回归算法模型；（具体的值）
- 分类算法：通常目标变量是分类型变量，比如预测性别（男、女）、预测颜色（红、白、黄）等，那就需要用分类算法去拟合得出分类算法模型。（某一类）

> 现有的机器学习系统类型繁多，为便于理解，主要依据以下几大标准将它们进行分类：
>
> - 是否在人类监督下训练（有监督学习，无监督学习，半监督学习和强化学习）
> - 是否可以动态地进行增量学习（在线学习和批量学习）
> - 是简单地将新数据点和已知的数据点进行匹配，还是像科学家那样，对训练数据进行模式检测然后建立一个预测模型（基于实例的学习和基于模型的学习）
>
> 这些标准之间互不排斥，你可以用喜欢的方式任意组合。例如，现在最先进的垃圾邮件过滤器可能是使用深度神经网络模型对垃圾邮件和常规邮件进行训练，完成动态学习。这使其成为一个在线的，基于模型的有监督学习系统。

## 2. 线性回归模型(Linear regression model)

线性回归与中学时代学习的“方程”非常相似，在结构上与一元一次方程和多元一次方程非常类似，因此可以使用这种角度进行学习线性回归模型会更加亲切。

线性回归模型是属于`有监督`的学习模型，其流程主要是试图学得一个**线性模型**，以尽可能准确地预测目标**实值**。具体而言则是：

> **给定一组输入样本，和每个样本对应的目标值，需要在某一损失准则下，找到（学习到）目标值和输入值的函数关系，这样，当有一个新的样本到达时，可以预测其对应的目标值是多少。**

更为概括地说，就是对输入特征加权求和，再加上一个偏置项（也称截距项）的常数进行预测。

公式如下：
$$
y' = h_\theta(x) = \theta_0 + \theta_1x_1 +  \theta_2x_2 +  ... +  \theta_nx_n
$$
其中：

- h~θ~（x）是预测值，h~θ~(x)往往会缩写成h(x)；
- n是特征的数量；
- xi是第几个特征值；
- θ~j~是第j个模型参数（θ~0~为偏置项，θ~j~为权重项）；

> :star: 注意：
>
> - 这里x是已知的，未知数是 θ~j~和 θ~0~。线性的对象指的其实是参数 θ~j~ 和 θ~0~（注意不是x）。
> - 模型主要依据于θ~1~这个权重项，而θ~0~主要是对整个结果进行微调。偏置项是(θ~0~)在训练模型过程中，为使得模型可以更精准一点而做的微调。

此外，线性回归依据特征数量的不同可以分为：**一元线性回归**与**多元线性回归**。

回到引例中，可得知：

- 特征：工资和年龄
- 目标（标签）：预测银行会贷款多少钱
- 参数：工资和年龄都会影响最终银行贷款的结果，那么它们各自都有多大的影响？
- 分类：一元线性回归（只依据工资）
- 模型：y = h~θ~(x) = θ~0~ +  θ~1~X~1~

<img src="01_线性回归与梯度下降.assets\image-20220501165407386.png" alt="image-20220501165407386" style="zoom: 33%;" />

## 3. 残差（residual）

即为拟合误差(Fitting error)，也就是**真实值和预测值之间的差值**。

由引例的模型和图像，不难看出，拟合直线或平面是无法完全精确的匹配每一个点的，只能尽可能得在杂乱中寻找规律，得到一个“大概”的值。用数学的模型去拟合现实的数据，这就是统计。统计不像数学那么精确，统计的世界不是非黑即白的，它有“灰色地带”，但是统计会将理论与实际间的差异表示出来，也就是“**误差**”。

可知在模型拟合的过程中必定会出现误差，但我们也希望最终得到的模型能够最大程度拟合所有的数据点，即希望**误差最小化**。用公式表示为：
$$
\varepsilon = y - y'，其中， y'为预测值
$$

## 4. 损失函数(Loss function)

损失函数（loss function）或[代价函数](https://baike.baidu.com/item/代价函数/7048599)（cost function），通常作为学习准则与优化问题相联系，即通过最小化损失函数求解和评估模型。

由于真实值与预测值是存在误差的，且每一个样本的误差值是不同的，因此可对每一个样本，计算其 **残差** 的值，再将其 **平方**（为了消除负号），将所有的 **残差** 相加，就量化出了拟合直线和实际之间的误差。公式如下：
$$
\operatorname{dist}(y_i, y_j)=\sum_{k=1}^{n}(y_{ik}-y_{jk})^{2}
$$
这个公式是**残差平方和**，即**`SSE`**(Sum of Squares for Error)，在机器学习中，它是最常用的**损失函数**（**衡量回归模型误差的函数，也就是选取出最优拟合所需的评价标准，这个值越小，说明模型拟合效果越好**）。

- 解释：由于误差有正有负，故使用平方和来抵消正负；

- 问题：

  - 使用平方后会放大(差>1)部分的误差，同时缩小(-1<差<1)部分的误差；

  - 当不同维度之间的度量差异很大时无法处理。

    > 例如：衡量一个人有年龄和收入两个维度，两个维度相差100倍以上，模型会严重受到收入大小的影响，要求在建模前对数据进行归一化处理；

衡量真实值预测值之间的差距，除了以上的SSE以外，还有许多其它的方法：**欧式距离判别，曼哈顿距离判别，马氏距离判别**等等。具体可参见：[机器学习(一)——线性回归_hhhcbw的博客](https://blog.csdn.net/weixin_44491423/article/details/121516466)

### 4.1 误差规律——独立同分布

独立同分布（`IID`，independently identically distribution）在概率统计理论中，指随机过程中，任何时刻的取值都为随机变量，如果这些随机变量服从同一分布，并且互相独立，那么这些随机变量是独立同分布。

- 误差ε(i)是**独立且具有相同分布**，并且服从均值为0，方差为θ^2^的高斯分布；
- **独立**：张三和李四一起来贷款，他俩没有关系，即每个样本到拟合平面的距离都不相同；
- **同分布**：他俩都来得是我们假定的同一家银行，所以它在预测的时候是按照同样的方式，数据是在同一个分布下去建模，尽可能来自相同的分布；
- **高斯分布**：银行可能会多给，也可能会少给，但是绝大多数情况下，这个浮动不会太大，极小情况下浮动会比较大（有的多有的少，大概来看，均值为0）。误差在0附近浮动的可能性较大，正负差距较大的可能性越来越小。符合概率统计中现实分布情况。

## 5. 最小二乘估计

假设银行贷款是依据依据我们的模型：y = h~θ~(x) = θ~0~ +  θ~1~X。可知只有 θ~0~ 与 θ~1~是未知的，我们现在的目的就是要去求出 θ~0~ 与 θ~1~。

由损失函数可知，我们要想求得最优的 θ~0~ 与 θ~1~，就需要让误差最小，即损失函数的值最小。

基于均方误差最小化来进行模型求解的方法称为“`最小二乘法`”。在线性回归中，**最小二乘法就是试图找到一条直线，使得所有样本到直线上的距离之和最小**。均方误差的公式：
$$
J({\theta}_{0},{\theta}_{1})=\frac{1}{n}\sum_{1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\frac{1}{n}\sum_{1}^{n}\left(y_{i}-\left(\hat{\theta}_{0}+\hat{\theta}_{1} x_{i}\right)\right)^{2}
$$
即原问题可转化为，求解J(θ~0~, θ~1~)最小时的 θ~0~ 与 θ~1~值。求解 θ~0~ 与 θ~1~使得残差平方和Q最小化的过程，称为线性回归模型的最小二乘“**参数估计**”(parameter estimation)。将J(θ~0~, θ~1~)对 θ~0~ 与 θ~1~分别求导，得到：
$$
\begin{array}{l}
\frac{\partial J_{(\theta_0, \theta_1)}}{\partial \theta_1}=2\left(\theta_1 \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-\theta_0\right) x_{i}\right) \\
\frac{\partial J_{(\theta_0, \theta_1)}}{\partial \theta_0}=2\left(m \theta_0-\sum_{i=1}^{m}\left(y_{i}-\theta_1 x_{i}\right)\right)
\end{array}
$$
再令其分别等于0，即可得到 θ~0~ 与 θ~1~最优解：
$$
\theta_1=\frac{\sum_{i=1}^{m} y_{i}\left(x_{i}-\bar{x}\right)}{\sum_{i=1}^{m} x_{i}^{2}-\frac{1}{m}\left(\sum_{i=1}^{m} x_{i}\right)^{2}} \\
\theta_0=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-w x_{i}\right) \\
其中\bar{x}=\frac{1}{m} \sum_{i=1}^{m} x_{i}  为 x 的均值
$$

## 6. 多元线性回归(Multivariate linear regression)

但实际上更一般的情形是，银行贷款不会仅由`工资`一个属性决定，当有多个属性的时候，上述求解方法便不再方便计算了，因此，为解决一般性问题，通常会将模型使用矩阵的方式表达，这也被称为“多元线性回归”
$$
f\left(\boldsymbol{x}_{i}\right)=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i}+b \text {, 使得 } f\left(\boldsymbol{x}_{i}\right) \simeq y_{i} \text {, }
$$
求解该问题（也就是最小二乘法）的方法一般为两种：**矩阵式**、**梯度下降法**。

### 6.1 矩阵式

类似的，可利用最小二乘法来对 θ~0~ 与 θ~1~进行估计，为便于讨论，我们把 θ~0~ 与 θ~1~吸收入向量形式 ==**θ** =(θ~1~;θ~0~)==，相应的，把数据集D表示为一个m *(d+1)大小的矩阵X，其中每行对应于一个示例，该行前d个元素对应于示例的d个属性值，最后一个元素恒置为1，即:
$$
\mathbf{X}=\left(\begin{array}{ccccc}
x_{11} & x_{12} & \ldots & x_{1 d} & 1 \\
x_{21} & x_{22} & \ldots & x_{2 d} & 1 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
x_{m 1} & x_{m 2} & \ldots & x_{m d} & 1
\end{array}\right)=\left(\begin{array}{cc}
\boldsymbol{x}_{1}^{\mathrm{T}} & 1 \\
\boldsymbol{x}_{2}^{\mathrm{T}} & 1 \\
\vdots & \vdots \\
\boldsymbol{x}_{m}^{\mathrm{T}} & 1
\end{array}\right)
$$
再把标记也写成向量形式 **y** =（y~1~；y~2~；...；y~m~），有:
$$
\hat{\boldsymbol{\theta}}^{*}=\underset{\hat{\theta}}{\arg \min }(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{\theta}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{\theta}})\\
令 E_{\hat{\boldsymbol{\theta}}}=(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{\theta}})^{\mathrm{T}}(\boldsymbol{y}-\mathbf{X} \hat{\boldsymbol{\theta}}) \\

则 \frac{\partial E_{\hat{\boldsymbol{\theta}}}}{\partial \hat{\boldsymbol{\theta}}}=2 \mathbf{X}^{\mathrm{T}}(\mathbf{X} \hat{\boldsymbol{\theta}}-\boldsymbol{y})
$$
令上式为零可得 **θ**最优解的闭式解, 但由于涉及矩阵逆的计算, 比单变量情形 要复杂一些。当 X^T^X 为满秩矩阵(full-rank matrix)或正定矩阵(positive definite matrix)时, 可得
$$
\hat{\boldsymbol{\theta}}^{*}=\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}
$$
其中(**X**^T^**X**)^-1^是矩阵(**X**^T^**X**)的逆矩阵。令 **x~i~** = (x~i~, 1)，则最终学得的模型为：
$$
f\left(\hat{\boldsymbol{x}}_{i}\right)=\hat{\boldsymbol{x}}_{i}^{\mathrm{T}}\left(\mathbf{X}^{\mathrm{T}} \mathbf{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{y}
$$
存在的问题：

- 不存在学习的过程；
- X^T^X 不是满秩矩阵。

> 现实任务中X^T^X 往往不是满秩矩阵。例如在许多任务中我们会遇到大量的变量?其数目甚至超过样例数，导致X 的列数多于行数， X^T^X 显然不满秩。此时可解出多个θ~1~， 它们都能使均方误差最小化。选择哪一个解作为输出，将由学习算法的归纳偏好决定， 常见的做法是引入正则化(regularization) 项。参见：[机器学习(一)——线性回归](https://blog.csdn.net/weixin_44491423/article/details/121516466)

### 6.2 梯度下降法(Gradient Descent)

梯度下降是[迭代法](https://baike.baidu.com/item/迭代法/10913188)的一种,可以用于求解[最小二乘问题](https://baike.baidu.com/item/最小二乘问题/19128644)(线性和非线性都可以)。在求解[机器学习算法](https://baike.baidu.com/item/机器学习算法/18635836)的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一。在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。**中心思想就是迭代地调整参数从而使损失函数最小化。**

对于线性回归而言，我们已知目标参数后是可以直接求解的，但这其实在机器学习中是一种例外，即大部分目标函数没办法通过数学方法计算求得，只能使用不停的迭代尽可能地一步步找到最优解。

> 举个通俗的例子来辅助理解：对于一元二次方程的求解方法，我们常用的有十字相乘法与公式法。对于部分方程，我们可以轻易的使用十字相乘法进行求解，但对于很复杂的方程，十字相乘法往往不那么容易求解。则，这时候则会使用公式法进行求解。
>
> :star: 需要注意的是，理论上只要有解，十字相乘法都是可以数学方法求解出来的，但对于最小二乘问题而言许多问题则是数学方法无法计算的，而梯度下降则是处理这种复杂的参数估计方法中常用的方法之一。

#### 6.2.1 梯度

在一元函数中叫做求导，在多元函数中就叫做**求梯度**。梯度下降是一个最优化算法，通俗的来讲也就是沿着梯度下降的方向来求出一个函数的极小值。比如一元函数中，加速度减少的方向，总会找到一个点使速度达到最小。

#### 6.2.2 梯度下降法

假设你迷失在山上的迷雾之中，你能感觉到的只有你脚下路面的坡度。快速到达山脚的一个策略就是沿着最陡的方向下坡。这就是梯度下降的做法：**随便选一个起点，计算损失函数对于参数矩阵在该点的偏导数，每次往偏导数的反向向走一步，步长通过 [公式] 来控制，直到走到最低点，即导数趋近于0的点为止。**

由于成本函数为：
$$
J({\theta}_{0},{\theta}_{1})=\frac{1}{n}\sum_{1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\frac{1}{n}\sum_{1}^{n}\left(y_{i}-\left(\hat{\theta}_{0}+\hat{\theta}_{1} x_{i}\right)\right)^{2}
$$
对其求偏导可得：
$$
\begin{array}{l}
\frac{\partial}{\partial \theta_{0}} J\left(\theta_{0}, \theta_{1}\right)= -\frac{2}{m} \sum_{i=1}^{m}\left(y^{(i)} - h_{\theta}\left(\boldsymbol{x}^{(i)}\right)\right) \\
\frac{\partial}{\partial \theta_{1}} J\left(\theta_{0}, \theta_{1}\right)= -\frac{2}{m} \sum_{i=1}^{m}\left(y^{(i)} - h_{\theta}\left(\boldsymbol{x}^{(i)}\right)\right) \boldsymbol{x}^{(i)}
\end{array}
$$
所以梯度下降的优化方法为：
$$
\begin{aligned}
\delta_{0} &:=-\frac{2 \alpha}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(\boldsymbol{x}^{(i)}\right)-y^{(i)}\right) \\
\delta_{1} &:=-\frac{2 \alpha}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(\boldsymbol{x}^{(i)}\right)-\boldsymbol{y}^{(i)}\right) x^{(i)} \\
\\
\theta_{0} &:=\theta_{0}+\delta_{0} \\
\theta_{1} &:=\theta_{1}+\delta_{1}
\end{aligned}
$$
其中**α**为学习率（步长），对结果会产生巨大的影响，调节学习率这个超参数是建模中的重要内容。

选择方法：从小的开始，不行再小。

存在的问题：

- 最小点时候收敛速度变慢；
- 并且对初始点的选择极为敏感。

> 梯度下降有时会陷入局部最优解的问题中，即下山的路上有好多小坑，运气不好掉进坑里，但是由于底部梯度(导数)也为0，故以为找到了山的底部。同时，步长选择的过大或者过小，都会影响模型的计算精度及计算效率。

#### 6.2.3 常用的梯度下降方法

- 批量梯度下降
- 随机梯度下降
- mini-batch梯度下降

##### 批量梯度下降

在每次更新时用所有样本。即在梯度下降中，对于 θ~i~的更新，所有的样本都有贡献，也就是参与调整 **θ** 。

- **优点：**其计算得到的是一个标准梯度，**对于最优化问题，凸问题，**也肯定可以达到一个全局最优。因而理论上来说一次更新的幅度是比较大的。如果样本不多的情况下，当然是这样收敛的速度会更快啦。

- **缺点：**很多时候，样本很多，更新一次要很久，这样的方法就不合适。

##### 随机梯度下降

在每次更新时随机使用1个样本来对参数进行更新

- **缺点：**用样本中的一个例子来近似所有的样本，来调整*θ*，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度。

- **优点：**对于**最优化问题**，**凸问题**，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。但是相比于批量梯度，这样的方法更快，**更快收敛**，虽然不是全局最优，但很多时候是我们可以接受的，所以这个方法用的也比批量梯度多。

##### mini-batch梯度下降

在每次更新时用b个样本，其实批量的梯度下降就是一种折中的方法，他用了一些小样本来近似全部的，其本质就是既然用1个样本可能不太准，那就用个30个50个样本，这样总会比随机的要准不少，而且批量的话还是非常可以反映样本的一个分布情况的。在深度学习中，这种方法用的是最多的。

批处理数量：32、64、128比较常用，很多时候还要考虑内存和效率。

- **优点：**这个方法收敛也不会很慢，收敛的局部最优也是更多的可以接受！

## 参考文献

- 逐步推导：[【机器学习实战】教程](https://www.bilibili.com/video/BV19b4y1Y7JQ)

- [机器学习实战：基于Scikit-Learn、Keras和TensorFlow：原书第2版](https://www.bilibili.com/read/cv13987612)
- [机器学习—线性回归算法](https://zhuanlan.zhihu.com/p/409261701)
- [损失函数_百度百科](https://baike.baidu.com/item/损失函数/1783236)
- [机器学习——线性回归算法](http://wjhsh.net/xiugeng-p-12977373.html)
- [机器学习—梯度下降](https://zhuanlan.zhihu.com/p/410360601)